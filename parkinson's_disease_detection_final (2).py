# -*- coding: utf-8 -*-
"""Parkinson's Disease Detection final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zjTlIhXc7FsoHFl3-mccp8daPjwtxBc4

Importing the Dependencies
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.metrics import accuracy_score

"""Data Collection & Analysis"""

# loading the data from csv file to a Pandas DataFrame
parkinsons_data = pd.read_csv('/content/parkinsons.csv')

# printing the first 5 rows of the dataframe
parkinsons_data.head()

# number of rows and columns in the dataframe
parkinsons_data.shape

# getting more information about the dataset
parkinsons_data.info()

# checking for missing values in each column
parkinsons_data.isnull().sum()

# getting some statistical measures about the data
parkinsons_data.describe()

# distribution of target Variable
parkinsons_data['status'].value_counts()

"""1  --> Parkinson's Positive

0 --> Healthy

"""



"""Data Pre-Processing

Separating the features & Target
"""

X = parkinsons_data.drop(columns=['name','status'], axis=1)
Y = parkinsons_data['status']

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
print(X)

print(Y)

"""Splitting the data to training data & Test data"""

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
print(X.shape, X_train.shape, X_test.shape)

"""Data Standardization"""

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
scaler = StandardScaler()

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
scaler.fit(X_train)

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
X_train = scaler.transform(X_train)

X_test = scaler.transform(X_test)

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
print(X_train)

"""Model Training

Support Vector Machine Model
"""

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
model = svm.SVC(kernel='linear')

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
# training the SVM model with training data
model.fit(X_train, Y_train)

"""Model Evaluation

Accuracy Score
"""

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
# accuracy score on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
print('Accuracy score of training data : ', training_data_accuracy)

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
# accuracy score on training data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)

import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
print('Accuracy score of test data : ', test_data_accuracy)

"""Building a Predictive System"""

input_data = (214.28900,260.27700,77.97300,0.00567,0.00003,0.00295,0.00317,0.00885,0.01884,0.19000,0.01026,0.01161,0.01373,0.03078,0.04398,21.20900,0.462803,0.664357,-5.724056,0.190667,2.555477,0.148569)

# changing input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the numpy array
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
# Fit the scaler with feature names
scaler.fit(X_train)

# Transform new data without feature names
X_test_scaled = scaler.transform(X_test)

# standardize the data
std_data = scaler.transform(input_data_reshaped)

prediction = model.predict(std_data)
print(prediction)


if (prediction[0] == 0):
  print("The Person does not have Parkinsons Disease")

else:
  print("The Person has Parkinsons")

import pickle

model = pickle.load(open('Data Science Final.sav', 'rb'))

filename = "Data Science Final.sav"
pickle.dump(parkinsons_data, open(filename, 'wb'))

import tensorflow as tf

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(32, activation='relu', input_dim = 22))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
model.summary()

# Add these missing imports at the beginning of the code
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn import svm
import numpy as np

# Use the correct variable names for training and testing data
model = svm.SVC(kernel='linear')
model.fit(X_train, Y_train)

X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)
print('Accuracy score of training data : ', training_data_accuracy)

X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)
print('Accuracy score of test data : ', test_data_accuracy)

# Correct the variable names for scaling input data
scaler.fit(X_train)
std_data = scaler.transform(input_data_reshaped)

# Load the saved model using pickle
model = pickle.load(open('Data Science Final.sav', 'rb'))

# Define the neural network model using Keras
model_nn = tf.keras.models.Sequential()
model_nn.add(tf.keras.layers.Dense(32, activation='relu', input_dim=22))
model_nn.add(tf.keras.layers.Dense(32, activation='relu'))
model_nn.add(tf.keras.layers.Dense(1, activation='sigmoid'))
model_nn.summary()

model_nn.compile(loss='binary_crossentropy', metrics=['accuracy'])
model_nn.fit(X_train, Y_train, batch_size=32, epochs=200, validation_data=(X_test, Y_test))